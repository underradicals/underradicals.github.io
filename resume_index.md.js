import{c as l,o as s,j as a,r as d,t as i,a as n,G as r,ae as p,w as c}from"./framework.js";const u={class:"person-name-wrapper"},h={class:"person-name"},b={class:"subtitle"},g={href:"mailto:{{ email }}"},f={href:"tel:+{{ phone }}"},w={__name:"ResumeHeader",props:["email","phone"],setup(t){return(o,e)=>(s(),l("div",u,[a("h1",h,[d(o.$slots,"person_name")]),a("h5",b,[d(o.$slots,"subtitle")]),a("p",null,[a("a",g,i(t.email),1)]),a("p",null,[e[0]||(e[0]=n(" Hubbard, TX ",-1)),a("a",f,i(t.phone),1)])]))}},y={class:"space-between-wrapper"},P={class:"space-between"},k={class:"no-margin"},v={class:"no-margin"},m={__name:"SpaceBetween",props:["left","right"],setup(t){return(o,e)=>(s(),l("section",y,[a("div",P,[a("p",k,i(t.left),1),a("p",v,i(t.right),1)]),e[0]||(e[0]=a("p",{class:"no-margin"},"University of Arkansas Grantham",-1))]))}},T=JSON.parse(`{"title":"Joseph's Resume","description":"The totality of my hard work, blood, sweat, and tears reduced to 1+ pages to be judged by some stranger, as if I am not human, or equal to one.","frontmatter":{"layout":"doc","title":"Joseph's Resume","description":"The totality of my hard work, blood, sweat, and tears reduced to 1+ pages to be judged by some stranger, as if I am not human, or equal to one.","sidebar":false},"headers":[],"relativePath":"resume/index.md","filePath":"resume/index.md"}`),x={name:"resume/index.md"},_=Object.assign(x,{setup(t){return(o,e)=>(s(),l("div",null,[r(w,{email:"underradicals@gmail.com",phone:"9035199940"},{person_name:c(()=>e[0]||(e[0]=[n(" Joseph Burton ",-1)])),subtitle:c(()=>e[1]||(e[1]=[n(" Aspired Engineer and Developer ",-1)])),_:1}),e[2]||(e[2]=a("h2",{id:"objective",tabindex:"-1"},[n("Objective "),a("a",{class:"header-anchor",href:"#objective","aria-label":'Permalink to "Objective"'},"​")],-1)),e[3]||(e[3]=a("hr",null,null,-1)),e[4]||(e[4]=a("blockquote",null,[a("p",null,[a("em",null,"Driven dedicated Software Engineer and Developer with a strong foundation in Information Technology and Computer Science. My experience ranges across backend and front-end application development, data engineering, data analytics with a background in logistics and commercial construction. I have a Master of Science in IT, and I hold a Bachelor of Science in Computer Science. I am looking for a challenging role that will leverage my technical and analytical skills, leadership experience and my ability to work through complex domains in which I have little or no experience. I am smart, fast learner and want nothing more than to help you reach your project goals.")])],-1)),e[5]||(e[5]=a("h2",{id:"education",tabindex:"-1"},[n("Education "),a("a",{class:"header-anchor",href:"#education","aria-label":'Permalink to "Education"'},"​")],-1)),e[6]||(e[6]=a("hr",null,null,-1)),r(m,{left:"Master of Science in Information Technology",right:"Expected: August 2025"}),r(m,{left:"Bachelor of Science in Computer Science",right:"Commencement: November 2023"}),e[7]||(e[7]=p('<h2 id="experience" tabindex="-1">Experience <a class="header-anchor" href="#experience" aria-label="Permalink to &quot;Experience&quot;">​</a></h2><hr><h3 id="r-n-transportation" tabindex="-1"><em>R&amp;N Transportation</em> <a class="header-anchor" href="#r-n-transportation" aria-label="Permalink to &quot;_R&amp;N Transportation_&quot;">​</a></h3><hr><h5 id="data-pipeline" tabindex="-1">Data Pipeline <a class="header-anchor" href="#data-pipeline" aria-label="Permalink to &quot;Data Pipeline&quot;">​</a></h5><blockquote><p><em>October</em> 2023 ― Present</p></blockquote><blockquote><p><em>Lead Software Engineer and Developer</em></p></blockquote><ul><li>Implemented data warehouse expansion with Sr Data Analyst, modeled R&amp;N data flow by normalizing ingestion up to isomorphism with respect to our schema to increase data availability and accuracy; as a result, R&amp;N was able to forecast and make better predictions on their future returns.</li><li>Aggregated and simplified data validation and transformations with Sr. Data Analyst, through optimized SQL queries, tools like dbt and Prefect for orchestration, this resulted in a more robust pipeline, mitigating data loss and ingestion failures by 100%.</li><li>Implemented data retention layer with Sr Data Analyst, by updating Change Data Capture layer to logical decoding migrating away from SQL triggers that produced fewer stable returns due to lag associated with triggers, as a result the ELT experienced more stable E2E performance gains.</li></ul><h5 id="r-n-admin-panel" tabindex="-1">R&amp;N Admin Panel <a class="header-anchor" href="#r-n-admin-panel" aria-label="Permalink to &quot;R&amp;N Admin Panel&quot;">​</a></h5><ul><li>Designed and Developed Administrative UI under the supervision of Project Sponsor, modeled ingested data, developed view compositions, and implemented the tie-in with back-end proprietary API, with micro-frontend (built in Typescript and React) error boundaries, and authentication. As a result, R&amp;N Transportation was able to track revenues, and able to maintain accurate employee information.</li><li>Design and Developed Company Database under supervision of Project Sponsor, domains included Compliance, HR, payroll, ELD logs and more; as a result, R&amp;N was able to save money and ultimately pay their drivers more and offer them better benefits.</li></ul><h5 id="r-n-client-api" tabindex="-1">R&amp;N Client API <a class="header-anchor" href="#r-n-client-api" aria-label="Permalink to &quot;R&amp;N Client API&quot;">​</a></h5><ul><li>Implemented REST API, under the coordination of the Project Sponsor, using the .NET Core Framework and C#, on-premises output-caching, rate-limiting, filtering, pagination, free observability via Aspire; as a result, R&amp;N was able to track company data in near-real-time and employees were able to stay within compliance limits.</li><li>Implemented REST API Gateway/Load-Balancer using .NET Core Framework, created NAT rules for SSH and TELNET access to API behind the LB, health checks, LB rules, as a result R&amp;N experienced near 100% uptime, and mitigated API catastrophic failure and increased workload limit from 10,000 rps, to 30,000+ rps. shoring R&amp;N for years into the future.</li></ul><h2 id="projects" tabindex="-1">Projects <a class="header-anchor" href="#projects" aria-label="Permalink to &quot;Projects&quot;">​</a></h2><h3 id="d2-etl" tabindex="-1"><em>D2 ETL</em> <a class="header-anchor" href="#d2-etl" aria-label="Permalink to &quot;_D2 ETL_&quot;">​</a></h3><hr><h5 id="project-management" tabindex="-1">Project Management <a class="header-anchor" href="#project-management" aria-label="Permalink to &quot;Project Management&quot;">​</a></h5><blockquote><p><em>August 2023 ― October 2023</em></p></blockquote><blockquote><p><em>Jr Data Engineer / Web Developer</em></p></blockquote><ul><li>Created deliverables Project-Charter, Project Management Plan and WBS. This created a clearly defined project scope and gave the product direction and unlike earlier project mitigated scope creep.</li><li>Rewrote D2 API, replacing custom pipeline with dbt and prefect, de-complecting the data model I was able to add analytics and more measures to the warehouse. This increased the richness of the data thereby enriching the potential of every consumer down the pipeline.</li></ul><h3 id="rest-api" tabindex="-1">REST API <a class="header-anchor" href="#rest-api" aria-label="Permalink to &quot;REST API&quot;">​</a></h3><blockquote><p><em>May 2019 ― August 2019</em><em>Jr Data Engineer / Jr Web Developer</em></p></blockquote><h5 id="project-execution" tabindex="-1">Project Execution <a class="header-anchor" href="#project-execution" aria-label="Permalink to &quot;Project Execution&quot;">​</a></h5><ul><li>Created ELT Data Model in PostgreSQL with data provided by Destiny2 manifest, created data pipeline that consumed manifest data, extracted blobs of Json into minIO s3 compliant buckets (lakehouse), built 3NF database (warehouse), extracted from Json blobs from minIO, transformed, normalized, and transported to 3NF database.</li><li>Built REST API in .NET Core that consumed the warehouse data with rate-limiting, filtering, and pagination. I was able to maintain upper bound of 10,000+ rps.</li><li>Built front-end web application in React to consume the REST API. API key authentication was used on M2C communication and Client Credential Flow was used on M2M communication.</li></ul>',23))]))}});export{T as __pageData,_ as default};
