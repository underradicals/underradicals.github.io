import{_ as e,c as t,o as r,ae as i}from"./chunks/framework.U1Gow_7P.js";const f=JSON.parse('{"title":"PySpark and Databricks","description":"","frontmatter":{"layout":"doc"},"headers":[],"relativePath":"resume/spark-and-databricks.md","filePath":"resume/spark-and-databricks.md"}'),o={name:"resume/spark-and-databricks.md"};function n(s,a,d,l,h,u){return r(),t("div",null,a[0]||(a[0]=[i('<h1 id="pyspark-and-databricks" tabindex="-1">PySpark and Databricks <a class="header-anchor" href="#pyspark-and-databricks" aria-label="Permalink to &quot;PySpark and Databricks&quot;">​</a></h1><h2 id="answers" tabindex="-1">Answers <a class="header-anchor" href="#answers" aria-label="Permalink to &quot;Answers&quot;">​</a></h2><ul><li><h4 id="dataframe-vs-rdd" tabindex="-1">DataFrame vs RDD: <a class="header-anchor" href="#dataframe-vs-rdd" aria-label="Permalink to &quot;DataFrame vs RDD:&quot;">​</a></h4><ul><li>When to use each, performance tradeoffs.</li></ul></li></ul><blockquote><p>You can think of a dataframe as a dictionary of key and value pairs where the key is a string and the value is a list of elements of type [T]. A <code>RDD</code> is Resilient Distributed Dataset. That means:</p></blockquote><ul><li>That an RDD is fault tolerant and can be recovered if a partition is a lost in case of node failures; this ensures data integrity and continued processing.</li><li>The Data inside a RDD is partitioned and distributed across multiple nodes in a cluster.</li><li>A DataSet is just a collection of elements, like an array or list, but designed for distributed environments.</li></ul><h4 id="other-facts-not-covered-in-the-acronym-are-immutability-and-deferred-execution" tabindex="-1">Other facts not covered in the acronym are immutability and Deferred Execution. <a class="header-anchor" href="#other-facts-not-covered-in-the-acronym-are-immutability-and-deferred-execution" aria-label="Permalink to &quot;Other facts not covered in the acronym are immutability and Deferred Execution.&quot;">​</a></h4><ul><li>RDD is a Monad. RDD&#39;s are immutable, that means once a RDD is created, its contents cannot be changed. Any operation of a RDD results in a new RDD, hence the Monadic nature.</li><li>Transformation on RDD&#39;s are executed on a deferred evaluation policy: That means they are not executed immediately but rather built into a graph of operations.</li></ul><p>In short, A RDD is a Monadic Automaton distributed over a cluster governed by a deferred execution policy.</p><h4 id="when-to-use-rdd" tabindex="-1">When to use RDD <a class="header-anchor" href="#when-to-use-rdd" aria-label="Permalink to &quot;When to use RDD&quot;">​</a></h4><blockquote><p>When the data does not fit into a tabular format</p></blockquote><h4 id="when-to-use-dataframe" tabindex="-1">When to use DataFrame <a class="header-anchor" href="#when-to-use-dataframe" aria-label="Permalink to &quot;When to use DataFrame&quot;">​</a></h4><blockquote><p>When ever the data is structured csv, json etc.</p></blockquote><h3 id="shuffle" tabindex="-1">Shuffle <a class="header-anchor" href="#shuffle" aria-label="Permalink to &quot;Shuffle&quot;">​</a></h3><p>A shuffle is the moving of data between partitions,</p><h3 id="narrow-transformations-vs-wide-transformations" tabindex="-1">Narrow Transformations vs Wide Transformations <a class="header-anchor" href="#narrow-transformations-vs-wide-transformations" aria-label="Permalink to &quot;Narrow Transformations vs Wide Transformations&quot;">​</a></h3><blockquote><p>A narrow transformation is one in which data result does not require a shuffle. A wide transformation is one is which the data result does require a shuffle.</p></blockquote><h3 id="optimizing-spark-shuffling" tabindex="-1">Optimizing Spark Shuffling <a class="header-anchor" href="#optimizing-spark-shuffling" aria-label="Permalink to &quot;Optimizing Spark Shuffling&quot;">​</a></h3><p>Let the tool do what it is best at, and spark is great at distributed compute. Not so great at distributed aggregation. So what is the solution? Let spark handle the narrow transformations, cache the result in an intermediary storage, like DuckDB, then let databases do what they are good at, which aggregating data.</p>',18)]))}const m=e(o,[["render",n]]);export{f as __pageData,m as default};
