import{_ as a,c as i,o as t,ae as o}from"./chunks/framework.U1Gow_7P.js";const u=JSON.parse('{"title":"Data Engineer Interview Prep","description":"","frontmatter":{"layout":"doc"},"headers":[],"relativePath":"resume/data-engineer.md","filePath":"resume/data-engineer.md"}'),n={name:"resume/data-engineer.md"};function r(s,e,l,c,d,g){return t(),i("div",null,e[0]||(e[0]=[o('<h1 id="data-engineer-interview-prep" tabindex="-1">Data Engineer Interview Prep <a class="header-anchor" href="#data-engineer-interview-prep" aria-label="Permalink to &quot;Data Engineer Interview Prep&quot;">â€‹</a></h1><h2 id="_1-core-technical-areas-to-review" tabindex="-1">1. <strong>Core Technical Areas to Review</strong> <a class="header-anchor" href="#_1-core-technical-areas-to-review" aria-label="Permalink to &quot;1. **Core Technical Areas to Review**&quot;">â€‹</a></h2><h3 id="ðŸ”¹-pyspark-databricks" tabindex="-1">ðŸ”¹ PySpark &amp; Databricks <a class="header-anchor" href="#ðŸ”¹-pyspark-databricks" aria-label="Permalink to &quot;ðŸ”¹ PySpark &amp; Databricks&quot;">â€‹</a></h3><ul><li><strong>DataFrame vs RDD</strong>: When to use each, performance tradeoffs.</li><li><strong>Wide vs Narrow Transformations</strong>: e.g., <code>map</code> (narrow) vs <code>groupBy</code>/<code>join</code> (wide).</li><li><strong>Shuffle operations</strong>: why theyâ€™re expensive, how to minimize.</li><li><strong>Partitioning strategies</strong>: <code>repartition</code>, <code>coalesce</code>, skew handling.</li><li><strong>Caching &amp; Persistence</strong>: when to use <code>cache()</code> vs <code>persist()</code> vs <code>checkpoint()</code>.</li><li><strong>UDFs vs Pandas UDFs</strong>: vectorized operations, when theyâ€™re worth using.</li><li><strong>Delta Lake</strong>: time travel, ACID transactions, schema evolution.</li></ul><h3 id="ðŸ”¹-cloud-migration-data-engineering" tabindex="-1">ðŸ”¹ Cloud Migration &amp; Data Engineering <a class="header-anchor" href="#ðŸ”¹-cloud-migration-data-engineering" aria-label="Permalink to &quot;ðŸ”¹ Cloud Migration &amp; Data Engineering&quot;">â€‹</a></h3><ul><li><strong>Data Lake vs Data Warehouse</strong>: roles in the architecture, migration considerations.</li><li><strong>ETL vs ELT in Cloud</strong>: how Databricks fits into both.</li><li><strong>Data ingestion patterns</strong>: batch vs streaming, ingestion from legacy DBs to cloud.</li><li><strong>File formats</strong>: Parquet, ORC, Avro, JSON â€” tradeoffs (compression, schema evolution).</li><li><strong>Performance tuning in cloud</strong>: autoscaling clusters, job clusters vs interactive clusters, cost optimization.</li></ul><hr><h2 id="_2-likely-technical-interview-questions" tabindex="-1">2. <strong>Likely Technical Interview Questions</strong> <a class="header-anchor" href="#_2-likely-technical-interview-questions" aria-label="Permalink to &quot;2. **Likely Technical Interview Questions**&quot;">â€‹</a></h2><ol><li>Explain how Spark executes a job (DAG, stages, tasks).</li><li>What are common performance bottlenecks in PySpark, and how do you fix them?</li><li>How would you handle skewed data in a large join?</li><li>If youâ€™re migrating an on-prem Oracle warehouse to cloud (Databricks + AWS/Azure), whatâ€™s your high-level strategy?</li><li>How do Delta Lake features (ACID, schema enforcement, time travel) help in migration scenarios?</li><li>You have a slow Spark job â€” what steps do you take to debug and optimize it?</li><li>Whatâ€™s the difference between <code>repartition()</code> and <code>coalesce()</code>? When would you use each?</li><li>How would you design an ELT pipeline for ingesting financial transactions daily?</li></ol><hr><h2 id="_3-system-design-practical-scenarios" tabindex="-1">3. <strong>System Design / Practical Scenarios</strong> <a class="header-anchor" href="#_3-system-design-practical-scenarios" aria-label="Permalink to &quot;3. **System Design / Practical Scenarios**&quot;">â€‹</a></h2><p>Expect scenario-based design problems:</p><ul><li><p><strong>Cloud migration</strong>:</p><ul><li>How would you migrate historical + incremental data from Oracle/DB2 to cloud (S3/ADLS â†’ Databricks â†’ Delta Lake â†’ downstream)?</li><li>How do you ensure <strong>data consistency</strong> during migration? (checkpoints, validation, reconciliation).</li></ul></li><li><p><strong>Financial data</strong>:</p><ul><li>Model payments, invoices, transactions in a normalized form.</li><li>Handle slowly changing dimensions (SCD Type 2 with Delta Lake).</li></ul></li><li><p><strong>Data quality</strong>:</p><ul><li>Implement validations (row counts, null checks, schema drift detection).</li></ul></li></ul><hr><h2 id="_4-key-prep" tabindex="-1">4. <strong>Key Prep</strong> <a class="header-anchor" href="#_4-key-prep" aria-label="Permalink to &quot;4. **Key Prep**&quot;">â€‹</a></h2><ul><li>They emphasize <strong>governance and compliance</strong>: be ready to talk about data lineage, GDPR/CCPA handling, audit logs.</li><li>They use <strong>Databricks on AWS and Azure</strong>: know storage (S3, ADLS), IAM roles, and networking basics.</li><li>They are heavy on <strong>Delta Lake</strong>: know how it fits with structured streaming and batch processing.</li><li>They care about <strong>cost management</strong>: cluster sizing, spot instances, job scheduling.</li></ul><hr><h2 id="_5-mock-rapid-fire-drill" tabindex="-1">5. <strong>Mock Rapid-Fire Drill</strong> <a class="header-anchor" href="#_5-mock-rapid-fire-drill" aria-label="Permalink to &quot;5. **Mock Rapid-Fire Drill**&quot;">â€‹</a></h2><ul><li>What is a Spark DAG?</li><li>Difference between narrow vs wide transformations?</li><li>How do you optimize a join between a huge and a small dataset?</li><li>Explain the benefits of Delta Lake.</li><li>Whatâ€™s your approach to cloud migration with minimal downtime?</li></ul>',19)]))}const p=a(n,[["render",r]]);export{u as __pageData,p as default};
