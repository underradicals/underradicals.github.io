import{_ as a,c as t,o as i,ae as s}from"./chunks/framework.U1Gow_7P.js";const r="/assets/spark_arch.B_lfr-jd.svg",k=JSON.parse('{"title":"Apache Spark","description":"","frontmatter":{"layout":"doc"},"headers":[],"relativePath":"docs/spark/index.md","filePath":"docs/spark/index.md"}'),o={name:"docs/spark/index.md"};function n(l,e,h,d,p,c){return i(),t("div",null,e[0]||(e[0]=[s('<h1 id="apache-spark" tabindex="-1">Apache Spark <a class="header-anchor" href="#apache-spark" aria-label="Permalink to &quot;Apache Spark&quot;">​</a></h1><h2 id="what-is-spark" tabindex="-1">What is Spark <a class="header-anchor" href="#what-is-spark" aria-label="Permalink to &quot;What is Spark&quot;">​</a></h2><p>Apache Spark is a unified computing engine for parallel data processing over a collection of two or more interconnected computers working together as a single unified system. These individual computers are called nodes, and the collection of nodes is called a cluster. This act of distributing workloads across multiple nodes, a cluster can improve fault tolerance, ensuring that if one node fails another can take over and continue operating on the job with minimal interruption.</p><h2 id="how-does-spark-work" tabindex="-1">How does spark work? <a class="header-anchor" href="#how-does-spark-work" aria-label="Permalink to &quot;How does spark work?&quot;">​</a></h2><p>Spark has two primitives. The RDD, which stands for A <strong>Residual Distributed Dataset</strong>, and the <strong>Distributed Variable</strong>. First lets talk a little about what is a RDD. Plainly, a RDD is Residual Distributed Dataset, (just kidding)...A RDD is represents an immutable partitioned collection of records that can parallelized. There are two types of distributed shared variables are Broadcast Variables and Accumulators. Broadcast variables allow you to efficiently share a large, immutable value (like a lookup table) to all worker nodes, preventing repeated network transmissions. Accumulators enable safe aggregation of information across all tasks in a distributed manner, which is useful for counting or summing values and can only be updated by tasks and read by the driver.</p><div class="info custom-block"><p class="custom-block-title">NOTE</p><p>Running tasks in parallel, is not the same thing as running tasks concurrently. Parallel does a collection of things at the same time while Concurrency manages a collection of things together, but only is every doing one task at a time.</p></div><p>Every time you run Spark, there are the same actors involved.</p><ul><li>The Driver Program</li><li>The Cluster Manager</li><li>Executors</li><li>Tasks</li><li>Shuffles (optional)</li><li>Stages</li><li>RDD&#39;s</li></ul><p><img src="'+r+`" alt="Spark Architecture"></p><h3 id="the-driver" tabindex="-1">The Driver <a class="header-anchor" href="#the-driver" aria-label="Permalink to &quot;The Driver&quot;">​</a></h3><p>The Driver process sits on the master node, runs your main function, and has three responsibilities:</p><div class="tip custom-block"><p class="custom-block-title">Driver Responsibilities</p><ol><li>Maintaining State of the Spark Application</li><li>Responding to the users program or input</li><li>Analysis, distribution, and scheduling work across the <strong>executors</strong>.</li></ol></div><p>The Drive acts as a controller in software development parlance, it is the entry point of the Spark Application. It must communicate with the Cluster Manager to get physical resources and manage executors. But on the simplest level, The Driver is just a process on a physical machine that is responsible for the Spark Application&#39;s state running on the cluster.</p><h3 id="executors" tabindex="-1">Executors <a class="header-anchor" href="#executors" aria-label="Permalink to &quot;Executors&quot;">​</a></h3><p>The executors are responsible for preforming the work that the Driver Program assigns. The Executor is responsible for only two things:</p><div class="tip custom-block"><p class="custom-block-title">Executor Responsibilities</p><ol><li>Executing Code</li><li>Reporting State of its computation back to the Driver Program.</li></ol></div><p>The executors are the processes that perform the tasks assigned by the Spark Driver. The executor has one job: Do the task given to it, and return the state of that task after it finishes.</p><h3 id="the-cluster-manager" tabindex="-1">The Cluster Manager <a class="header-anchor" href="#the-cluster-manager" aria-label="Permalink to &quot;The Cluster Manager&quot;">​</a></h3><p>The Cluster Manager is responsible for, well, managing a cluster of machines. These machines are broken into two types: Master and worker nodes.</p><h2 id="dataframes" tabindex="-1">DataFrames <a class="header-anchor" href="#dataframes" aria-label="Permalink to &quot;DataFrames&quot;">​</a></h2><p>In modern Spark one of the core primitives is the <code>DataFrame</code>. What is a DataFrame? Well in python you can think of it as a dictionary of lists, where each list is a value associated with a key.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">df </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;id&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;name&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;John&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Mary&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Kelly&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;age&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">24</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">22</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">32</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><blockquote><p>This example of a dataframe is naive. A DataFrame is more complex than this. But for this stage of understanding <code>Spark</code> it will give you some fundamental conceptual foundation on which to build.</p></blockquote><p>Now think of this in contrast to a table which is how data is organized in a database, at least from a query perspective.</p><table tabindex="0"><thead><tr><th>id</th><th>name</th><th>age</th></tr></thead><tbody><tr><td>1</td><td>John</td><td>24</td></tr><tr><td>2</td><td>Mary</td><td>22</td></tr><tr><td>3</td><td>Kelly</td><td>32</td></tr></tbody></table><h3 id="benefits-of-using-a-dataframe" tabindex="-1">Benefits of using a DataFrame <a class="header-anchor" href="#benefits-of-using-a-dataframe" aria-label="Permalink to &quot;Benefits of using a DataFrame&quot;">​</a></h3><p>What would a database have to do in order to query for say the name of every person in a User table? Well it would need to do what is called a <code>Scan</code>, which is naively where it has to iterate over every row in the database. If the Table has millions of rows, this gets very expensive.</p><p>However for a DataFrame, as you see this is as simple as <code>df[&quot;name&quot;]</code>. We went from an <code>O(n)</code> operation to <code>O(1)</code></p><h2 id="second-primitive" tabindex="-1">Second Primitive <a class="header-anchor" href="#second-primitive" aria-label="Permalink to &quot;Second Primitive&quot;">​</a></h2><p>The second primitive of modern Spark is the Monad RDD which is short for (Resilient Distributed Dataset). The RDD is a immutable partitioned collection of elements distributed across a cluster.</p><blockquote><h4 id="qualities-of-an-rdd" tabindex="-1">Qualities of an RDD <a class="header-anchor" href="#qualities-of-an-rdd" aria-label="Permalink to &quot;Qualities of an RDD&quot;">​</a></h4><ul><li>Resilient: Spark tracks the sequence of transformations so that in the event a partition is lost, Spark can recompute on failure.</li><li>Distributed: Data is divided into partitions, each potentially on a different node.</li><li>DataSet: The RDD holds elements of type <code>T</code></li><li>Immutable: All transformations produce a new RDD, they are never modified in place.</li></ul></blockquote><p>I placed this second, but in reality all other API&#39;s like <code>DataFrames</code> and <code>DataSets</code> are compiled down to RDD&#39;s internally before execution. The RDD is built on a deferred execution architecture, by that what is meant, the RDD does not compute anything until what Spark calls an <code>Action</code> has been requested by the caller.</p><h3 id="rdd-lineage" tabindex="-1">RDD Lineage <a class="header-anchor" href="#rdd-lineage" aria-label="Permalink to &quot;RDD Lineage&quot;">​</a></h3><p>RDD lineage is the recording of the sequence of transformations that produced the RDD. It is stored as a DAG (Directed Acyclic Graph) of RDD&#39;s where each node in the DAG represents an RDD and each edge represents a transformation like (<code>map</code>, <code>filter</code> <code>join</code> etc...)</p>`,34)]))}const m=a(o,[["render",n]]);export{k as __pageData,m as default};
