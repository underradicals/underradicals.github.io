import{_ as e,c as t,o as s,ae as n}from"./chunks/framework.U1Gow_7P.js";const o="/assets/elt-process.Delpe2pi.png",p=JSON.parse('{"title":"The Modern Data Architecture","description":"","frontmatter":{"layout":"doc"},"headers":[],"relativePath":"data_engineering/modern-data-architecture.md","filePath":"data_engineering/modern-data-architecture.md"}'),r={name:"data_engineering/modern-data-architecture.md"};function i(d,a,l,c,h,u){return s(),t("div",null,a[0]||(a[0]=[n('<h1 id="the-modern-data-architecture" tabindex="-1">The Modern Data Architecture <a class="header-anchor" href="#the-modern-data-architecture" aria-label="Permalink to &quot;The Modern Data Architecture&quot;">​</a></h1><p>Within organizations, teams often build data solutions in isolation and have their own data ingestion, storage, management, and governance layers. This leads to delays and increases the cost of data-driven decisions. It also prevents the deeper insights that come when an organization analyzes all their relevant data together.</p><p>To avoid these challenges, you must build a modern data architecture for analytics and insights that breaks down all data silos—including third-party data. This architecture creates end-to-end governance and puts the data in the hands of everyone in the organization.</p><p>A modern data strategy uses technology building blocks to help you manage, access, analyze, and act on your data.</p><p>The building blocks roughly correspond to a basic workflow. Choose the numbered markers in the following diagram for descriptions of each stage.</p><p><img src="'+o+'" alt="Animation"></p><h2 id="data-lake" tabindex="-1">Data Lake <a class="header-anchor" href="#data-lake" aria-label="Permalink to &quot;Data Lake&quot;">​</a></h2><h3 id="store-phase" tabindex="-1">[<strong>Store Phase</strong>] <a class="header-anchor" href="#store-phase" aria-label="Permalink to &quot;[**Store Phase**]&quot;">​</a></h3><p>Before you can ingest data, you need a place to put it, therefore a modern data architecture starts with the data lake. A data lake is a centralized repository that you can use store structured, semi-structured, and unstructured data at scale. Organizations can use it to ingest, store, and analyze diverse datasets without the need for extensive preprocessing.</p><p><strong>Amazon S3</strong> provides an optimal foundation for a data lake because of its virtually unlimited scalability and high durability. You can seamlessly and non-disruptively increase storage from gigabytes to petabytes of content and only pay for what you use.</p><div class="tip custom-block"><p class="custom-block-title">What is a Silo</p><p>Independent data stores that are optimized for specific uses and are difficult to combine or access by other systems</p></div><h2 id="specialized-ingest-services" tabindex="-1">Specialized ingest services <a class="header-anchor" href="#specialized-ingest-services" aria-label="Permalink to &quot;Specialized ingest services&quot;">​</a></h2><h3 id="ingest-phase" tabindex="-1">[<strong>Ingest Phase</strong>] <a class="header-anchor" href="#ingest-phase" aria-label="Permalink to &quot;[**Ingest Phase**]&quot;">​</a></h3><h4 id="aws-dms" tabindex="-1">AWS DMS <a class="header-anchor" href="#aws-dms" aria-label="Permalink to &quot;AWS DMS&quot;">​</a></h4><p>Use AWS DMS to load data from relational and non-relational databases.</p><h4 id="aws-data-firehose" tabindex="-1">AWS Data Firehose <a class="header-anchor" href="#aws-data-firehose" aria-label="Permalink to &quot;AWS Data Firehose&quot;">​</a></h4><p>Ingest real-time data streams with Amazon Data Firehose. Convert your data stream into formats such as Apache Parquet or ORC, decompress the data, or perform custom data transformations.</p><h4 id="aws-msk-managed-streaming-for-kafka" tabindex="-1">AWS MSK (Managed Streaming for Kafka) <a class="header-anchor" href="#aws-msk-managed-streaming-for-kafka" aria-label="Permalink to &quot;AWS MSK (Managed Streaming for Kafka)&quot;">​</a></h4><p>With Amazon MSK, build fully managed Apache Kafka clusters for real-time streaming data pipelines and applications.</p><h4 id="aws-iot-core" tabindex="-1">AWS IoT Core <a class="header-anchor" href="#aws-iot-core" aria-label="Permalink to &quot;AWS IoT Core&quot;">​</a></h4><p>Connect billions of IoT devices and route trillions of messages to AWS services with AWS IoT Core.</p><h4 id="aws-datasync" tabindex="-1">AWS DataSync <a class="header-anchor" href="#aws-datasync" aria-label="Permalink to &quot;AWS DataSync&quot;">​</a></h4><p>Use AWS DataSync to transfer data from on-premises file shares, object storage systems, and Hadoop clusters to AWS storage services. Synchronize on a scheduled basis.</p><h4 id="transfer-family" tabindex="-1">Transfer Family <a class="header-anchor" href="#transfer-family" aria-label="Permalink to &quot;Transfer Family&quot;">​</a></h4><p>Automate file transfers into and out of Amazon S3 using SFTP, FTPS, and SFTP protocols with the Transfer Family.</p><h4 id="aws-snow-family" tabindex="-1">AWS Snow Family <a class="header-anchor" href="#aws-snow-family" aria-label="Permalink to &quot;AWS Snow Family&quot;">​</a></h4><p>In cases where transfer through a network is not feasible because of data volume or sensitivity, use the AWS Snow Family of purpose-built physical devices.</p><h2 id="cataloging-service" tabindex="-1">Cataloging service <a class="header-anchor" href="#cataloging-service" aria-label="Permalink to &quot;Cataloging service&quot;">​</a></h2><p>An essential component of a data lake built on Amazon S3 is the data catalog. Organizations can use cataloging to keep track of data assets and understand what data exists, where it is located, its quality, and how it is used. A data catalog is designed to provide a single source of truth about the contents of the data lake.</p><h3 id="aws-glue-data" tabindex="-1">AWS Glue Data <a class="header-anchor" href="#aws-glue-data" aria-label="Permalink to &quot;AWS Glue Data&quot;">​</a></h3><p>AWS Glue Data Catalog creates a catalog of metadata about your stored assets. Use this catalog to help search and find relevant data sources based on various attributes like name, owner, business terms, and others.</p><div class="tip custom-block"><p class="custom-block-title">What is the Data Analytics Workflow</p><p>Ingest → Store → Catalog → Process → Deliver</p></div><h2 id="processing-services" tabindex="-1">Processing services <a class="header-anchor" href="#processing-services" aria-label="Permalink to &quot;Processing services&quot;">​</a></h2><p>After the data is cataloged, it can now be processed or transformed into formats that are more useful for analysis and insights. Transformation can include data type conversion, filtering, aggregation, standardization, and normalizing.</p><h3 id="use-aws-glue" tabindex="-1">Use AWS Glue <a class="header-anchor" href="#use-aws-glue" aria-label="Permalink to &quot;Use AWS Glue&quot;">​</a></h3><p>Use AWS Glue, a fully managed extract, transform, and load (ETL) service, to prepare and cleanse data from various sources for analysis. It helps classify data, extract schema, and populate data catalogs.</p><h3 id="amazon-emr" tabindex="-1">Amazon EMR <a class="header-anchor" href="#amazon-emr" aria-label="Permalink to &quot;Amazon EMR&quot;">​</a></h3><p>With Amazon EMR, process big datasets using open-source frameworks, customized Amazon Elastic Compute Cloud (Amazon EC2) clusters, Amazon Elastic Kubernetes Service (Amazon EKS), AWS Outposts, or Amazon EMR Serverless. It can be used to run batch jobs for data processing.</p><h3 id="amazon-managed-service-apache-flink" tabindex="-1">Amazon Managed Service (Apache Flink) <a class="header-anchor" href="#amazon-managed-service-apache-flink" aria-label="Permalink to &quot;Amazon Managed Service (Apache Flink)&quot;">​</a></h3><p>Quickly author SQL code for real-time data processing using Amazon Managed Service for Apache Flink. Tasks include filtering, aggregating, joining, and deriving.</p><h2 id="analytics-services" tabindex="-1">Analytics services <a class="header-anchor" href="#analytics-services" aria-label="Permalink to &quot;Analytics services&quot;">​</a></h2><p>Transformed data is delivered to consumers and stakeholders, such as data scientists, data analysts, and business analysts. The primary purpose of data analytics is to extract insights from data that can lead to good business or organizational outcomes. Many AWS services can be used at this stage.</p><h3 id="amazon-redshift" tabindex="-1">Amazon Redshift <a class="header-anchor" href="#amazon-redshift" aria-label="Permalink to &quot;Amazon Redshift&quot;">​</a></h3><p>Amazon Redshift can directly analyze large sets of structured data across many functional databases and datasets without moving the data.</p><h3 id="athena" tabindex="-1">Athena <a class="header-anchor" href="#athena" aria-label="Permalink to &quot;Athena&quot;">​</a></h3><p>Athena queries large datasets directly on Amazon S3 using standard SQL syntax, in various formats such as CSV, Parquet, and ORC.</p><h3 id="amazon-emr-1" tabindex="-1">Amazon EMR <a class="header-anchor" href="#amazon-emr-1" aria-label="Permalink to &quot;Amazon EMR&quot;">​</a></h3><p>Use Amazon EMR to run analytics frameworks like Apache Spark, Hive, Presto, and Flink on large datasets stored in AWS services like Amazon S3 and Amazon DynamoDB. Examples include log analysis, machine learning, data science, web indexing and scientific simulations.</p><h3 id="amazon-databases" tabindex="-1">Amazon databases <a class="header-anchor" href="#amazon-databases" aria-label="Permalink to &quot;Amazon databases&quot;">​</a></h3><p>Use more than fourteen purpose-built Amazon databases to store, query, and analyze large datasets. Choose from relational, key-value, document, in-memory, graph, time series, wide column, and ledger databases.</p><h3 id="opensearch" tabindex="-1">OpenSearch <a class="header-anchor" href="#opensearch" aria-label="Permalink to &quot;OpenSearch&quot;">​</a></h3><p>Deploy, operate, and scale OpenSearch clusters in the AWS Cloud. Analyze large volumes of data from various sources like Amazon Kinesis Data Streams, Amazon S3, and Amazon DynamoDB using the OpenSearch APIs.</p><h3 id="amazon-quicksight" tabindex="-1">Amazon QuickSight <a class="header-anchor" href="#amazon-quicksight" aria-label="Permalink to &quot;Amazon QuickSight&quot;">​</a></h3><p>Visualize and analyze large datasets using SQL, charts, graphs, and dashboards with Amazon QuickSight.</p><h3 id="sagemaker" tabindex="-1">SageMaker <a class="header-anchor" href="#sagemaker" aria-label="Permalink to &quot;SageMaker&quot;">​</a></h3><p>Build, train, and deploy machine learning models for use in predictive analytics, computer vision for image recognition, natural language processing, recommendation systems, and more.</p><div class="tip custom-block"><p class="custom-block-title">Why is a Data Catalog Important?</p><p>A data catalog acts as a single source of truth for metadata, and tracks data location and quality.</p></div><h2 id="security-and-governance" tabindex="-1">Security and governance <a class="header-anchor" href="#security-and-governance" aria-label="Permalink to &quot;Security and governance&quot;">​</a></h2><p><strong>Security</strong> in data analytics systems refers to measures taken to protect data from unauthorized access, breaches, or attacks. It involves safeguarding data confidentiality, integrity, and availability. The entire data analytics system depends on data being secured and accessible only by authorized users. <strong>Governance</strong> encompasses the policies, procedures, and processes that ensure the proper management, quality, and use of data. It involves defining roles, responsibilities, and decision-making processes related to data. Following are some of the AWS services used for security and governance. These are covered in more detail in this course in the <strong>Security and Monitoring in Data Analytics Systems</strong> lesson.</p><h3 id="lake-formation" tabindex="-1">Lake Formation <a class="header-anchor" href="#lake-formation" aria-label="Permalink to &quot;Lake Formation&quot;">​</a></h3><p>With Lake Formation, you can centrally manage and scale fine-grained data access permissions and share data with confidence within and outside your organization.</p><h3 id="iam" tabindex="-1">IAM <a class="header-anchor" href="#iam" aria-label="Permalink to &quot;IAM&quot;">​</a></h3><p>IAM manages fine-grained access and permissions for human users, software users, other services, and microservices.</p><h3 id="aws-kms" tabindex="-1">AWS KMS <a class="header-anchor" href="#aws-kms" aria-label="Permalink to &quot;AWS KMS&quot;">​</a></h3><p>Use AWS KMS to create and control data encryption keys for data at rest and in transit.</p><h3 id="macie" tabindex="-1">Macie <a class="header-anchor" href="#macie" aria-label="Permalink to &quot;Macie&quot;">​</a></h3><p>Use Macie to automatically discover, classify, and protect sensitive data in AWS, such as personally identifiable information (PII).</p><h3 id="amazon-datazone" tabindex="-1">Amazon DataZone <a class="header-anchor" href="#amazon-datazone" aria-label="Permalink to &quot;Amazon DataZone&quot;">​</a></h3><p>Use Amazon DataZone to catalog, discover, share, and govern data stored across AWS, on premises, and third-party sources.</p><h3 id="audit-manager" tabindex="-1">Audit Manager <a class="header-anchor" href="#audit-manager" aria-label="Permalink to &quot;Audit Manager&quot;">​</a></h3><p>Audit Manager continuously audits usage to assess risk and compliance with regulations and industry standards.</p>',71)]))}const g=e(r,[["render",i]]);export{p as __pageData,g as default};
